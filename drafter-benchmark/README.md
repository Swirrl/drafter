# Drafter Benchmarks

Project containing benchmarks for the core drafter draftset operations - append, delete, publish, delete graph and
SPARQL Update. The project uses [JMH](https://github.com/openjdk/jmh) to generate a harness for building and running
benchmarks.

## Configuring and building

The project is defined using [Maven](https://maven.apache.org/index.html). Once maven has been installed the project can 
be built with:

    mvn clean verify
    
This will output an uberjar capable of running the benchmarks to `target/benchmarks.jar`.

List all available benchmark with `java -jar target/benchmarks.jar -l`

### Configuring

The benchmark harness has a dependency on the drafter jar to benchmark. This jar must either exist in the local maven 
repository, or exist in a configured remote repository it can be fetched from. When working locally, a drafter version 
can be installed into the local repository by executing the following the drafter source directory:

    clj -T:build install

Maven projects are defined by their `pom.xml` file. Once the drafter jar has been installed, it can be benchmarked by
updating the drafter dependency within `pom.xml`:

__pom.xml__
```language xml
<dependencies>
  <dependency>
    <groupId>com.swirrl</groupId>
    <artifactId>drafter</artifactId>
    <version>version-to-benchmark</version>
  </dependency>
</dependencies>
```

After rebuilding the benchmarking jar, this can then be executed on the benchmarking machine (see [Running](#running) below)

## Test data

The benchmarks all use the same set of test data which are parameterised along three dimensions:

* The total number of statements
* The total number of graphs the statements are contained within
* The percentage of 'graph-referencing' statements within the data

A graph-referencing statement is one which contains a graph in one or more of the subject, predicate or object positions e.g.

__example.trig__
```format trig
graph1 {
  <http://s1> <http://p1> graph1  # references its own graph
  graph1 <http://p2> graph1       # multiple references
}

graph2 {
  graph1 <http://p3> "o3" .       # references a different graph
}
```

The test data is randomly generated in a collection of files which vary the values of the above three parameters among
the following values:

* 1k, 10k, 100k and 1m (1000k) statements
* 1, 10, 100 and 200 graphs
* 0%, 1%, 5% and 10% graph-referencing statements

Each data file indicates the values of these parameters in the file name. Data file names follow the format

    data_[statements]k_[graphs]g_[graph-referencing]pc.nq
    
e.g. the file `data_100k_10g_5pc.nq` is a data file containing 100k statements over 10 graphs with 5% of the statements
referencing a graph.

Each data file also has a corresponding 'delete' file which contains a subset of the data in the data file. These are
used by the delete benchmarks to delete a known subset of the source data. These files all follow the format

    data_file.delete

e.g. `data_100k_10g_5pc.nq.delete` contains a subset of the data from the `data_100k_10_5pc.nq` file.

### Generating test data

Test data is randomly generated by the `data-gen` project. This can be used to generate a single file or output all
required data files to an output directory.

A single file can be generated with e.g.:

    clj -M -m data-gen.main generate -s 20000 -g 3 -r 2% -o data_20k_3g_2pc.nq
    
This will output a test data file containing 20k statements over 3 graphs where 2% of the generated statements 
reference a graph.

All data files can be generated to an output directory with:

    clj -M -m data-gen.main generate-all -o output-directory

## Benchmark layout

The benchmarks are defined across benchmark classes for each drafter operation i.e. append, delete, delete graph, publish
and SPARQL update. Within these classes, individual benchmark methods are defined which correspond to one of the test
data files.

JMH allows benchmark methods to take a 'state' class parameter which defines setup and teardown operations to be performed
around benchmark iteration and invocations. State classes must have a parameterless constructor so they can be instantiated
at runtime. Benchmarks are logically parameterised by the input data file(s) they depend on, so each one requires its own
state class which can provide the names of these files. The pattern used in most benchmark classes is therefore to define
an abstract base state class containing the required parameters (usually a single test data file) and which defines the
setup and teardown methods. Each benchmark then has its own subclass providing the files used by the benchmark in its
default constructor. These state subclasses are then consumed as parameters in the corresponding benchmark methods. Each
benchmark then delegates to a core benchmark method defined in terms of the base state class.

### Graph-referencing benchmarks

Each of the three data file dimensions (number of statements, number of graphs and percentage of graph-referencing statements)
currently have four possible values, resulting in 64 possible combinations. Since most benchmarks require their own state
class this leads to an explosion in the number of possible benchmarks as these dimensions increase in size. In order to limit
the number of required benchmarks, the number of graph-referencing benchmarks is limited.

Most benchmarks fix the percentage of graph-referencing triples to 0% and only vary the number of statements and graphs.
There are three benchmarks for each operation which fix the number of statements and graphs to 100k and 10 respectively and then
varies the graph-referencing percentages. This allows some indication on the effect of increasing the percentage of 
graph-referencing statements on each operation. 

### Large delete benchmarks

The delete benchmarks of 1m triples reliably cause stardog to crash by running out of heap space so these are temporarily
disabled.

## Running

The benchmarks execute various commands against stardog using the `stardog-admin` command, so they require the location
of the stardog install directory to be configured. This will probably be `/opt/stardog` on a test server or 
`.omni_cache/install/stardog/install/` when running locally. They also need the location of the test data directory 
generated by `data-gen`. Once the benchmarks jar has been built, the benchmarks can be run with:

    java -Dstardog.dir=stardog/directory -Ddata.dir=data/directory -jar target/benchmarks.jar [benchmark_filter]
    
The optional `benchmark_filter` can be specified to restrict the benchmarks to run. For example all append tests can be
run with:

    java -Dstardog.dir=stardog/directory -Ddata.dir=data/directory -jar target/benchmarks.jar appendTest
    
In order to generate charts from the benchmark results, the results should be written to a CSV file. This is done with
the `-rf` option:

    java -Dstardog.dir=stardog/directory -Ddata.dir=data/directory -jar target/benchmarks.jar -rf csv

this will write the results to a jmh-result.csv file after all benchmarks have completed.

## Displaying and comparing results

Once benchmarks have been run and their results collected into a CSV file (see [Running](#running) above), the results
can be used to generate charts showing how performance changes as the number of statements, graphs or percentage of
graph-referencing statements increased.

These graphs can be generated using the `perf-charts` project:

    clj -M -m perf-charts.main -d output-directory jmh-result-version1.csv [jmh-result-version2.csv ...]
    
`perf-charts` reads a collection of benchmark result CSV files, groups the results by the number of statements, graphs 
and percentage of referential statements and visualise how the operation times are affected as these increase while the other
two dimensions remain constant. The input file names should have the format `jmh-result-[version].csv` indicating
the version of drafter which was used to generate the benchmark results.

The chart images are written to the specified output directory, and the names of the output files indicate the fixed 
dimensions used to produce the chart. The output file names are of the following format:

    [benchmark_name]-([statements]k)?-([graphs]g)?-([ref-statements])?.nq
    
The output file will contain two of the statement, graph or ref-statements components which indicates these values
were kept constant while the missing component varies. For example the file `append-100k-0pc.png` shows results for
the `append` benchmark to append 100000 statements where 0% are graph-referencing while the number of graphs increases.

Each chart contains a plot for each of the versions provided so performance of each operation can be compared if 
corresponding benchmark results were available in each benchmark results file.